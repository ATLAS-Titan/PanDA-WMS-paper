The PanDA  system was developed to meet the scale and complexity of LHC
distributed computing for the ATLAS experiment.  In the process,  the old batch
job paradigm  of computing in HEP was  discarded  in favor of a  far more
flexible and scalable  model. The success  of PanDA  at the LHC is leading to
widespread adoption and testing by other experiments. PanDA  is the first
exascale  workload management system in HEP, already operating at a million
computing jobs per day, and processing over an exabyte of data in 2013. Next LHC
run will pose massive computing  challenges. With a  doubling of the beam
energy  and luminosity as  well as an increased  need for  simulates  data, the
data volume is expected to increase with a factor 5--6 or more. Storing and
processing  this amount of data is a  challenge   that cannot be resolved with
the currently existing  computing  resources in ATLAS\@. To resolve this
challenge, ATLAS is turning to commercial  as well as academic Cloud services
and HPCs via the PanDA system. Also the work underway is enabling the use of
PanDA by new scientific collaborations and communities as a means  of leveraging
extreme scale computing  resources with a low barrier of entry. The technology
base provided by the PanDA system will enhance the usage of a variety  of
high-performance computing resources available to basic research.

HEP was amongst the first, if not the very first experimental community to
realize the importance of using WMS to manage their computational campaign(s).
As computing become increasingly critical for a range of other experiments,
the impact of WMS on HEP foreshadows the importance of WMS for other
experiments and their computational campaigns. Other experiments such as SKA,
LSST etc.., will have their own workload characteristics, resources types and
federation constraints, as well metrics of performance.  The design and
experience captured in this paper should proof invaluable for computational
campaigns that need to evaluate the relative merits of different approaches.

Thanks to the efforts captured in this paper, the time-averaged utilization of
Titan has consistently been upwards of 90\%. Compare this to the time averaged
utilization of other similar leadership machines such as NSF's flagship Blue
Waters, where the average utilization fluctuates between 60-80\% (XDMoD)
machine~\cite{bw-sucks}. These impressive if not unprecedented operation gains
aside, this work is just a starting point towards more effective operational
models for future leadership and online analytical platforms\cite{foap-url},
which will have to support ever increasing complex workloads with varying
models for dynamic resource federation. Recent efforts with Google Cloud for
the CMS experiment -- HEPCloud~\cite{hepcloud,google-hep} represent similar
yet different approaches to dynamic resource federation.

Our experience demonstrated the advantages arising from a  systems engineering
and abstractions based approach, in particular the pilot abstraction. As the
type, heterogeneity and complexity of future platforms  (e.g., Exascale,
Commercial Clouds) for science increases, there will be a greater need and
emphasis on abstractions to ensure WMS can both utilize as well as support
platforms.







