The PanDA  system was developed to meet the scale and complexity of LHC
distributed computing for the ATLAS experiment.  In the process,  the old batch
job paradigm  of computing in HEP was  discarded  in favor of a  far more
flexible and scalable  model. The success  of PanDA  at the LHC is leading to
widespread adoption and testing by other experiments. PanDA  is the first
exascale  workload management system in HEP, already operating at a million
computing jobs per day, and processing over an exabyte of data in 2013. Next LHC
run will pose massive computing  challenges. With a  doubling of the beam
energy  and luminosity as  well as an increased  need for  simulates  data, the
data volume is expected to increase with a factor 5--6 or more. Storing and
processing  this amount of data is a  challenge   that cannot be resolved with
the currently existing  computing  resources in ATLAS\@. To resolve this
challenge, ATLAS is turning to commercial  as well as academic Cloud services
and HPCs via the PanDA system. Also the work underway is enabling the use of
PanDA by new scientific collaborations and communities as a means  of leveraging
extreme scale computing  resources with a low barrier of entry. The technology
base provided by the PanDA system will enhance the usage of a variety  of
high-performance computing resources available to basic research.
