Experiments at the Large Hadron Collider (LHC) face unprecedented computing
challenges. Heterogeneous resources are distributed worldwide, thousands of
physicists analyzing the data need remote access to hundreds of computing
sites, the volume of processed data is beyond the exabyte scale, and data
processing requires more than billions of hours of computing usage per year. The
PanDA (Production and Distributed Analysis) system was developed to meet the
scale and complexity of LHC distributed computing for the ATLAS experiment. In
the process, the old batch job paradigm of computing in HEP was discarded in
favor of a far more flexible and scalable model. The success of PanDA at the LHC
is leading to widespread adoption and testing by other experiments. PanDA is
the first exascale workload management system in HEP, already operating at
a million computing jobs per day, and processing over an exabyte of data in
2013. We will describe the design and implementation of PanDA, present data on
the performance of PanDA at the LHC, and discuss plans for future evolution
of the system to meet new challenges of scale, heterogeneity and increasing
user base.
