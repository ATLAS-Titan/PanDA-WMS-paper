% Experiments at the Large Hadron Collider (LHC) face unprecedented computing
% challenges. Heterogeneous resources are distributed worldwide, thousands of
% physicists analyzing the data need remote access to hundreds of computing
% sites, the volume of processed data is beyond the exabyte scale, and data
% processing requires more than billions of hours of computing usage per year. The
% PanDA (Production and Distributed Analysis) system was developed to meet the
% scale and complexity of LHC distributed computing for the ATLAS experiment. In
% the process, the old batch job paradigm of computing in HEP was discarded in
% favor of a far more flexible and scalable model. The success of PanDA at the LHC
% is leading to widespread adoption and testing by other experiments. PanDA is
% the first exascale workload management system in HEP, already operating at
% a million computing jobs per day, and processing over an exabyte of data in
% 2013. We will describe the design and implementation of PanDA, present data on
% the performance of PanDA at the LHC, and discuss plans for future evolution
% of the system to meet new challenges of scale, heterogeneity and increasing
% user base.

% Experiments at the Large Hadron Collider (LHC) face unprecedented computing
% challenges. Thousands of physicists analyze exabytes of data every year, using
% billions of computing hours on hundreds of computing sites worldwide. PanDA
% (Production and Distributed Analysis) is a workload management system (WMS)
% developed to meet the scale and complexity of LHC distributed computing for the
% ATLAS experiment. PanDA is the first exascale workload management system in HEP,
% executing millions of computing jobs per day, and processing over an exabyte of
% data in 2016. In this paper, we introduce the design and implementation of
% PanDA, describing its deployment on Titan, the third biggest supercomputer in
% the world. We analyze scalability, reliability and performance of PanDA on
% Titan, highlighting the challenges addressed by its architecture and
% implementation. We present preliminary results of experiments performed with the
% Next Generation Executer, a prototype we developed to meet new challenges of
% scale and resource heterogeneity.


The computing systems used by LHC experiments has historically consisted of
the federation of hundreds to thousands of distributed resources,  ranging in
scale from small to mid-size resource. As the LHC prepares for Run 3 in
$~\approx$ 2022 and the high-luminosity era (Run 4), it is anticipated that
the data volumes that will need analyzing will increase by factors of 10-100
compared to the current phase (Run 2). Data will be larger in volume but will
also require more sophisticated computational processing.  In spite of the
impressive scale of the existing distributed computing solutions, the
federation of small to mid-size resources will be insufficient to meet
projected supply. This paper is a case study of how the ATLAS experiment has
embraced leadership class high-performance computing in conjunction with
traditional distributed high-throughput computing. Specifically, we discuss
the experience of integrating PanDA WMS with the DOE leadership computing
facility (Titan) to reach sustained production scales of approximately 51M
core-hours a years. This state-of-practice paper provides three main
contributions:  (i) a critical evaluation of the many design and operational
considerations that have been taken to support the sustained, scalable and
production usage of Titan for historically high-throughput workloads; (ii) a
preliminary characterization of a next generation executor   for PanDA to
support non- traditional heterogeneous workloads and advanced execution modes;
and (iii) early lessons and guidance as the community looks forward to
designing the next generation of online analytical platforms, for how current
and future experimental and observational systems can be integrated with
production supercomputers in a general and extensible manner.
