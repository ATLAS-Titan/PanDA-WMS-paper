% \ifreview
% Comments to address in this section:
% \begin{enumerate}
% 	\color{red} 
% 	\item REVIEWER 1: The introduction is overly verbose and prosaic. The
% 	praise for LHC collaborations is off-putting, especially as it ignores
% 	other work. For example, claiming "CMS and ATLAS experiments utilize
% 	arguably the largest production grade distributed computing solutions"
% 	disregards the obvious commercial contenders. By blatantly excluding
% 	obvious comparisons, it raises the question how much other work is
% 	ignored. \jhanote{Added "academic" after arguably. Any more?}
% \end{enumerate}
% \fi

The Large Hadron Collider (LHC) was created to explore the fundamental
properties of matter. Multiple experiments at LHC have collected and
distributed hundreds of petabytes of data worldwide to hundreds of computer
centers. Thousands of physicists analyze petascale data volumes daily. The
detection of the Higgs Boson in 2013 speaks to the success of the detector
and experiment design, as well as the sophistication of computing systems
devised to analyze the data, which historically, consisted of the federation
of hundreds to thousands of distributed resources, ranging in scale from
small to mid-size resource~\cite{foster2003grid}.

The LHC workloads are comprised of tasks that are independent of each other,
however, the management of the distribution of workloads across many
heterogeneous resources, the effective utilization of resources and efficient
execution of workloads present non-trivial challenges. Many software
solutions have been developed in response to these challenges. The CMS
experiment, devised a solution based around the
HTCondor~\cite{thain2005distributed} software ecosystem. The
ATLAS~\cite{Aad:2008} experiment utilizes the Production and Distributed
Analysis (PanDA) workload management system~\cite{Maeno2011} (WMS) for
distributed data processing and analysis. The CMS and ATLAS experiments
utilize, arguably the largest academic production grade distributed computing
solutions, and have symbolized the paradigm of {\it high-throughput
computing} (HTC), i.e., the effective execution of many independent tasks.

In spite of the impressive scale of the ATLAS distributed computing system --
in the number of tasks executed, the number of core hours utilized, and the
number of distributed sites utilized,  demand for computing systems will soon
significantly outstrip current and projected supply.   The data volumes that
will need analyzing in LHC-Run 3 ($\approx$ 2022) and the high-luminosity
era (Run 4) will increase by factors of 10--100 compared to the current phase
(Run 2). There are multiple levels at which this problem needs to be
addressed: the utilization of emerging parallel architectures (e.g.,
platforms); algorithmic and advances in analytical methods (e.g., use of
Machine Learning); and the ability to exploit different platforms (e.g.,
clouds and supercomputers).

% As the LHC prepares for Run 3 in $~\approx$ 2022 and the high-luminosity
% era (Run 4), it is anticipated that the data volumes that will need
% analyzing will increase by factors of 10-100 compared to the current phase
% (Run 2). Data will be larger in volume but will also require more
% sophisticated computational processing.

This paper represents the experience of how the ATLAS experiment has ``broken
free'' of the traditional computational approach of high-throughput computing
on distributed resources to embrace new platforms, in particular
high-performance computers (HPC). Specifically, we discuss the experience of
integrating PanDA WMS with a US DOE leadership machine (Titan) to reach
sustained production scales of approximately 51M core-hours a year.

In doing so, we demonstrate how Titan is more efficiently utilized by the
mixing of small and short-lived tasks in backfill with regular payloads.
Cycles otherwise unusable (or very difficult to use) are used for science,
thus increasing the overall utilization on Titan without loss of overall
quality-of-service. The conventional mix of jobs at OLCF cannot be
effectively backfilled because of size, duration, and scheduling policies.
Our approach is extensible to any HPC with ``capability scheduling''
policies.  We also investigate the use of a pilot-abstraction based task
execution runtime system to flexibly execute ATLAS and other heterogeneous
workloads (molecular dynamics) using regular queues. As such, our approach
provides a general solution and investigation of the convergence of HPC and
HTC execution of workloads.

This work demonstrates a viable production route to delivering large amounts
of computing resources to ATLAS and, in the future, to other experimental and
observational use cases.  This broadens the use of leadership computing while
demonstrating how distributed workflows can be integrated with leadership
resources, and effectively accommodating HTC and HPC workloads
simultaneously.

This paper also provides: (i) a critical evaluation of the many design and
operational considerations that have been taken to support the sustained,
scalable and production usage of Titan for historically high-throughput
workloads, and (ii) early lessons and guidance on designing the next
generation of online analytical platforms~\cite{foap-url},  so that
experimental and observational systems can be integrated with production
supercomputers in a general and extensible manner.
