The Production and Distributed Analysis (PanDA) is a Workload Management System
(WMS) developed to support the operations of the LHC ATLAS experiment. PanDA is
specifically design to manage the execution of distributed workloads and
workflows via pilots.

WMS for distributed executions are a type of middlware, designed to coordinate
specific activities about resources and workloads: discovering and selecting
resources, submitting the tasks of a workload, and monitoring the execution of
those tasks~\cite{marco2009glite}. Pilot is an abstraction that enables
multi-level scheduling by decoupling resource acquisition from workload
scheduling~\cite{turilli2015comprehensive}. Pilots are implemented by resource
placeholders: a request for a certain amount of resources is scheduled on a site
(e.g., by submitting a job or requesting a virtual machine) and once the
resources are acquired, tasks are scheduled directly to the pilot, not to site's
scheduler.

WMS like PanDA implement the pilot abstraction: WMS control the acquisition of
resources by means of pilots and then manage the execution of workloads or
workflows' tasks on those pilots. In tis way, pilots expose a uniform scheduling
interface and execution environment, abstracting the differences among the
heterogeneous resources of multiple sites; WMS centralize the management of the
execution process, offering a unified interface to the application layer.
Pilot-enabled WMS are particularly relevant for LHC experiments, where large
amount of heterogeneous resources, distributed across multiple sites have to be
coordinated to execute tens of million of tasks per months while producing and
storing petabytes of data.

Several pilot-enabled WMS were developed for the LHC experiments:
AliEn~\cite{Bagnasco2010} for ALICE; DIRAC~\cite{Paterson2010} for LHCb;
GlideinWMS~\cite{sfiligoi2008glideinwms} for CMS; and
PanDA~\cite{maeno2014evolution} for ATLAS. These systems implement similar
design and architectural principles: separation of concern between management
and execution, and data and compute; centralization of the management
capabilities; distributed execution of the execution capabilities across
multiple sites; unification of the application interface; hiding of resource
heterogeneity; centralized monitoring and accounting of both resources and
execution; and collection of static and sometimes dynamic information about
resources.

AliEn, DIRAC, GlideinWMS and PanDA all share a similar design with two types of
components: the management ones facing the application layer and centralizing
the capabilities required to acquire tasks' descriptions and matching them to
resource capabilities; and resource components required to acquire information
about resource capabilities and the resources themselves. Architecturally, the
management components include one or more queue and a scheduler capable of
coordinating via push/pull protocols with the resource modules. All resource
components include middleware-specific APIs for submitting resource requests
and, in the case of the pilot-enabled WMS of the LHC projects, a pilot capable
of pulling tasks from the management modules and scheduling them on their
resources (i.e., multi-level scheduling).

AliEn, DIRAC, GlideinWMS and PanDA present several similarities also in their
implementations. All were initially implemented to use Grid resources, using one
or more components to the Condor software ecosystem~\cite{thain2005distributed}
and, as with GlideinWMS, contributing to its development. Accordingly, all LHC
WMS implemented Grid-like authentication and authorization systems and adopted a
computational model based on distributing a large amount of single/few-cores
tasks across hundreds of sites\mtnote{Is this true?}. Tasks are assumed to fail
or being held and all WMS implements subsystems dedicated to manage these
failures or simply resubmitting failed tasks.

All the experiments at LHC produces and process large amount of data both from
actual collisions in the accelerator and from their simulations. Dedicated,
multi-tiered data systems have been built to store, replicate, and distributed
these data. All LHC WMS interface with these systems to move data to the sites
where related compute tasks are executed or to schedule compute tasks where
(large amount of) data are already stored.

Currently, the implementation of PanDA WMS has at least two distinguishing
features: the scale at which it manages computation, and network-aware
scheduling of tasks to sites. PanDA concurrently supports distributed
computations on up to 240,000 cores, submitting more than 365 million jobs in
2016 across more than 100 sites, and processing up to XXXXPB of data distributed
worldwide across XX Tier 1-3 facilities. As for 2017, PanDA serves several
thousand users, including XX production groups. The scale of this operation is
unprecedented, making PanDA the WMS managing the largest computational campaign
in the world.

PanDA was initially implemented to support computations across an unreliable and
relatively expensive networking infrastructure. Accordingly, PanDA assumed a
rigid model of data placement based on replication and long-term storage of data
on the Grid sites. This model proven too wasteful, requiring storage space for
unused and obsolete data. PanDA implementation was evolved to benefit from the
development of the internet and of the networking infrastructure for scientific
research. Today, PanDA constantly monitors network throughput and latency  among
its management components, Grid sites and data facilities. PanDA uses this
information to schedule tasks on sites, depending on the estimated time required
to download and/or replicate input data and to stage out output data.

The dynamic management of data replication and download is an example of a
generalized paradigm shift that distinguishes the implementation of PanDA from
other WMS. New components are being implemented to support the dynamic sizing of
input dataset based on resource capabilities, the dynamic sizing of the
resources held by pilots both in terms of number of cores and duration of the
pilot, and further abstraction of different types of resources to reconcile
High-Throughput and High-Performance Computing paradigms.

% \subsection{Terminology}
%
% \mtnote{To be moved to the end of the introduction.}


% Each task may have an arbitrary number of properties like number of cores,
% executables, or input/output files. Tasks may have precedence interrelations,
% depending on their data dependences or any other type of dependency mandated
% by the application algorithms. Tasks with precedence relations have to be
% executed serially; otherwise, tasks can be executed concurrently. A workload
% is defined as a set of tasks that can be executed concurrently. As such, a
% workflow can be composed of a set of workloads.

% The terms ``task'' and ``job'' are also used inconsistently across communities
% that perform scientific computing. In this paper, ``job'' refers to the unit of
% work that is submitted to a local resource management system (LRMS), like the
% Slurm or PBS batch system of a cluster. As such, a task can become a job when
% is scheduled on a resource that exposes a LRMS but can also become a virtual
% machine or a container when bootstrapped on an infrastructure supporting
% virtualization. Tasks can be statically or dynamically grouped into jobs,
% depending on the resource capabilities and the task requirements.
%
% Usually, workflows are represented as graphs in which tasks are vertices and
% relations are edges~\cite{}. Often, graphs are supposed to be acyclic but graphs
% with cycles have been used to represent workflows of workflows~\cite{}. In this
% paper, a workflow template is a type of graph while a workflow instance is a
% workflow template with specific vertices and edges. Further, a workflow instance
% is ``abstract'' when no resource properties are available for all vertices,
% ``concrete'' otherwise~\cite{}. When not qualified, the term ``workflow''
% indicates a abstract workflow instance.
%
% Notes for refactoring:
% \begin{itemize}
%     \item Naming and nomenclature is a contribution
%     \item 1, 1.5 pages.
% \end{itemize}
