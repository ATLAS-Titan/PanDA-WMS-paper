The Production and Distributed Analysis (PanDA) software is a Workload
Management System (WMS) developed to support the operations of the LHC ATLAS
experiment. PanDA is specifically design to manage the execution of distributed
workloads and workflows via pilots.

WMS for distributed executions are a type of middleware, designed to coordinate
specific activities about resources and workloads: discovering and selecting
resources, submitting tasks of workloads and workflows, and monitoring the
execution of those tasks~\cite{marco2009glite}. Pilot is an abstraction that
enables multi-level scheduling by decoupling resource acquisition from workload
scheduling~\cite{turilli2015comprehensive}. Pilots are implemented by resource
placeholders: a request for a certain amount of resources is scheduled on a site
(e.g., by submitting a job or requesting a virtual machine) and once the
resources are acquired, tasks are scheduled directly to the pilot, not to site's
scheduler.

WMS like PanDA implement the pilot abstraction: WMS control the acquisition of
resources by means of pilots and then manage the execution of workloads or
workflows' tasks on those pilots. WMS centralize the management of the execution
process, offering a unified interface to the application layer. Pilots expose a
uniform scheduling interface and execution environment, abstracting the
differences among the heterogeneous resources of multiple sites. Pilot-enabled
WMS are particularly relevant for LHC experiments, where millions of tasks are
executed across multiple sites every month, analyzing and producing petabytes of
data.

Several pilot-enabled WMS were developed for the LHC experiments:
AliEn~\cite{Bagnasco2010} for ALICE; DIRAC~\cite{Paterson2010} for LHCb;
GlideinWMS~\cite{sfiligoi2008glideinwms} for CMS; and
PanDA~\cite{maeno2014evolution} for ATLAS. These systems implement similar
design and architectural principles: centralization of task and resource
management, and of monitoring and accounting; distribution of task execution
across multiple sites; unification of the application interface; hiding of
resource heterogeneity; and collection of static and sometimes dynamic
information about resources.

AliEn, DIRAC, GlideinWMS and PanDA all share a similar design with two types of
components: the management ones facing the application layer and centralizing
the capabilities required to acquire tasks' descriptions and matching them to
resource capabilities; and resource components used to acquire compute and data
resources and information about their capabilities. Architecturally, the
management components include one or more queue and a scheduler that coordinates
with the resource modules via push/pull protocols. All resource components
include middleware-specific APIs to request for resources, and a pilot capable
of pulling tasks from the management modules and executing them on its
resources.

AliEn, DIRAC, GlideinWMS and PanDA also have similar implementations. These WMS
were initially implemented to use Grid resources, using one or more components
to the Condor software ecosystem~\cite{thain2005distributed} and, as with
GlideinWMS, contributing to its development. Accordingly, all LHC WMS
implemented Grid-like authentication and authorization systems and adopted a
computational model based on distributing a large amount of single/few-cores
tasks across hundreds of sites\mtnote{Is this true?}.

All the experiments at LHC produces and process large amount of data both from
actual collisions in the accelerator and from their simulations. Dedicated,
multi-tiered data systems have been built to store, replicate, and distributed
these data. All LHC WMS interface with these systems to move data to the sites
where related compute tasks are executed or to schedule compute tasks where
(large amount of) data are already stored.

Currently, the implementation of PanDA WMS has at least two distinguishing
features: the scale at which it operates distributed computing, and
network-aware scheduling of tasks to sites\mtnote{PanDA team: please let me know
whether more unique (i.e., not present in other LHC WMS) features of PanDA
implementation should be indicated}. PanDA concurrently supports distributed
computations on up to 240,000 cores, submitting more than 365 million jobs a
year across 100 sites, and processing up to XXXXPB of data distributed worldwide
on XX Tier 1-3 facilities. As for 2017, PanDA serves several thousand users,
including XX production groups. The scale of this operation is unprecedented,
making PanDA the WMS that manages the largest computational campaign in the
world.

PanDA was initially implemented to support computations across an unreliable and
relatively expensive networking infrastructure. Accordingly, PanDA assumed a
rigid model of data placement based on replication and long-term storage of data
on the Grid sites. This model proven too wasteful, requiring storage space for
unused and obsolete data. PanDA implementation was evolved to benefit from the
development of the internet and of the networking infrastructure for scientific
research. Today, PanDA constantly monitors network throughput and latency among
its management components, Grid sites and data facilities. PanDA uses this
information to schedule tasks on sites, depending on the estimated time required
to download and/or replicate input data and to stage out output data.

The dynamic management of data replication and download is an example of a
generalized paradigm shift that distinguishes the implementation of PanDA from
other WMS. New components are being implemented to support: (i) dynamic sizing
of input dataset based on resource capabilities; (ii) dynamic sizing of the
resources held by pilots both in terms of number of cores and duration of the
pilot; and (iii) further abstraction of different types of resources to
reconcile high-throughput and high-performance computing paradigms.

% \subsection{Terminology}
%
% \mtnote{To be moved to the end of the introduction.}

% Each task may have an arbitrary number of properties like number of cores,
% executables, or input/output files. Tasks may have precedence interrelations,
% depending on their data dependences or any other type of dependency mandated
% by the application algorithms. Tasks with precedence relations have to be
% executed serially; otherwise, tasks can be executed concurrently. A workload
% is defined as a set of tasks that can be executed concurrently. As such, a
% workflow can be composed of a set of workloads.

% The terms ``task'' and ``job'' are also used inconsistently across communities
% that perform scientific computing. In this paper, ``job'' refers to the unit
% of work that is submitted to a local resource management system (LRMS), like
% the Slurm or PBS batch system of a cluster. As such, a task can become a job
% when is scheduled on a resource that exposes a LRMS but can also become a
% virtual machine or a container when bootstrapped on an infrastructure
% supporting virtualization. Tasks can be statically or dynamically grouped
% into jobs, depending on the resource capabilities and the task requirements.

% Usually, workflows are represented as graphs in which tasks are vertices and
% relations are edges~\cite{}. Often, graphs are supposed to be acyclic but
% graphs with cycles have been used to represent workflows of
% workflows~\cite{}. In this paper, a workflow template is a type of graph
% while a workflow instance is a workflow template with specific vertices and
% edges. Further, a workflow instance is ``abstract'' when no resource
% properties are available for all vertices, ``concrete'' otherwise~\cite{}.
% When not qualified, the term ``workflow'' indicates a abstract workflow
% instance.

% Notes for refactoring:
% \begin{itemize}
%     \item Naming and nomenclature is a contribution
%     \item 1, 1.5 pages.
% \end{itemize}
