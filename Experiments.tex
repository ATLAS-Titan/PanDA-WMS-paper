% In the batch queue,  queue time becomes dominant but, at the same time, we
% have more freedom to decide the parameters of the slot.

\subsection{Experiments}

In this section we illustrate the performances of the NGE by means of experiments.
Experiments aim to determine the performances of the NGE and to show how it provides a negligible overhead while introducing new functionalities.

All the experiments have been performed on TITAN's batch queue. This allowed us to decide the pilot size in terms of number of nodes and wall-time. For the same reason, the completion time of each run of our experiments is dominated by the queue time. However, since we are interested only in the performances of the NGE, we present only data related to the execution time of the pilot and the tasks that are run within it.

We test the NGE pilot for weak scalability, weak scalability with resubmission, strong scalability and we provide a proof of concept about how the NGE is able to run heterogeous workloads.


\subsubsection{Weak scalability}
\aanote{I am using the term simulation....I will change it once we decide how to call the CU}
The experiment consists in running as many tasks as the number of nodes controlled by the pilot.
Tasks consists in Athena-MP job that simulate 100 events taken from ATLAS workload.
Each Athena-MP job uses 16 cores which coincide with the number of cores available on TITAN's nodes.
In order to minimize measurement noises, we used the very same simulation for all the tasks. Therefore, ideally all the tasks should have the same execution time which corresponds to ~$80$ minutes. 
 
Tasks do not experience queue within the pilot since there is one node for each ATLAS simulation. Therefore, delays are of only three factors: i) the bootstrapping of the pilot on the nodes; ii) the manager that has to dispatch tasks to the pilot; iii) the time that the agent requires to spawn all of simulations on the nodes.

We tested different pilot sizes, i.e. : 250, 500, 1000 and 2000. For all of them the walltime was 2 hours.

Figure \ref{fig:weakScal1} depicts the average pilot duration, the average execution time of each tasks and the average time required to send a tasks for execution.  


\begin{figure}[!htb]
        \includegraphics[width=0.5\textwidth]{./figures/NGE/Weak.pdf}
    \caption{Average pilot execution time, average tasks execution time and average launching time for pilot sizes: 250, 500, 1000 and 2000.}
\label{fig:weakScal1}
\end{figure}
\aanote{TO be commented as soon as the figure becomes definitive. Note that It would be better if we show the ratio between pilot duration and execution time}
\subsubsection{Weak scalability with resubmission }
This experiment is similar to the one presented above but in this case we want to test also the impact of submitting new simulations in place of those that end. Thus, we send to the pilot a number of tasks that is five times larger than the number of nodes. The reason behind this experiment is to stress the manager and the agent by pushing new tasks while others are ending their execution in such a way to increase the concurrency of these two components.

In order to perform the experiments in a reasonable amount of time, we reduced  the number of events that are simulated by Athena-MP to sixteen. This allows us to complete a task in ~25 minutes only. Note that the number of events has been chosen in such a way that all the cores of a node run one event.

We tested different pilot sizes, i.e. : 250, 500, 1000 and 2000. For all of them the walltime was 3 hours. 

Figure \ref{fig:weakScal2} \aanote{(DRAFT)}  depicts the average pilot duration, the average execution time of each tasks and the average time required to send a tasks for execution.  

\begin{figure}[!htb]
        \includegraphics[width=0.48\textwidth]{./figures/draft/AvgT.pdf}
    \caption{Average pilot execution time, average tasks execution time and average launching time for pilot sizes: 250, 500, 1000 and 2000.}
\label{fig:weakScal2}
\end{figure}

\subsubsection{Strong scability}

\subsubsection{Heterogeneous execution}






% For this reason, the second set of the experiments aims to find
%sub-optimal parameters with which we can minimize the trade-off between the size
%of a slot and the time spent in queue waiting for that slot to become available.
%In other words, we aim to minimize the completion time by finding the best
%trade-off between execution time and queue time.
%
%This execution model introduces slot utilization as one of the key factors for
%high-performances. This happens because, in order to minimize the time spent in
%queue, we might asks for slots in advance and, then we could not be able to
%saturate them when they become available. Thus, this strategy requires a new
%functionality that allows the job to receive and execute new events while it is
%already running on the resources. In order to do that we perform the experiments
%by using a new generation executor that implements such functionality.
%
%As last observation, it is important to point out that the percentage of
%utilization of a slot is minor problem with the current implementation because,
%due to the dynamics of the Backfill queue, PanDA has a high probability to
%re-acquire a slot immediately after it has released one\aanote{Are we able to
%quantify this ``immediately''?}.
